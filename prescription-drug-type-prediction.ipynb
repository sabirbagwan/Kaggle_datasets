{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-07T13:16:14.135401Z","iopub.execute_input":"2022-12-07T13:16:14.135975Z","iopub.status.idle":"2022-12-07T13:16:14.155610Z","shell.execute_reply.started":"2022-12-07T13:16:14.135936Z","shell.execute_reply":"2022-12-07T13:16:14.154692Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"/kaggle/input/memory-test-on-drugged-islanders-data/Islander_data.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:16:14.157651Z","iopub.execute_input":"2022-12-07T13:16:14.158310Z","iopub.status.idle":"2022-12-07T13:16:14.165658Z","shell.execute_reply.started":"2022-12-07T13:16:14.158273Z","shell.execute_reply":"2022-12-07T13:16:14.164343Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/memory-test-on-drugged-islanders-data/Islander_data.csv')","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:16:14.167494Z","iopub.execute_input":"2022-12-07T13:16:14.168114Z","iopub.status.idle":"2022-12-07T13:16:14.188550Z","shell.execute_reply.started":"2022-12-07T13:16:14.168079Z","shell.execute_reply":"2022-12-07T13:16:14.186922Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:16:14.190690Z","iopub.execute_input":"2022-12-07T13:16:14.191412Z","iopub.status.idle":"2022-12-07T13:16:14.213846Z","shell.execute_reply.started":"2022-12-07T13:16:14.191366Z","shell.execute_reply":"2022-12-07T13:16:14.212695Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"     first_name  last_name  age Happy_Sad_group  Dosage Drug  \\\n0       Bastian   Carrasco   25               H       1    A   \n1          Evan   Carrasco   52               S       1    A   \n2     Florencia   Carrasco   29               H       1    A   \n3         Holly   Carrasco   50               S       1    A   \n4        Justin   Carrasco   52               H       1    A   \n..          ...        ...  ...             ...     ...  ...   \n193       Jacob      Novak   52               H       3    T   \n194         Teo    Steiner   41               S       3    T   \n195   Alexander  Takahashi   54               S       3    T   \n196  Alexandere  Takahashi   40               H       3    T   \n197       Chloe  Takahashi   32               S       3    T   \n\n     Mem_Score_Before  Mem_Score_After  Diff  \n0                63.5             61.2  -2.3  \n1                41.6             40.7  -0.9  \n2                59.7             55.1  -4.6  \n3                51.7             51.2  -0.5  \n4                47.0             47.1   0.1  \n..                ...              ...   ...  \n193              71.3             74.3   3.0  \n194              72.5             70.4  -2.1  \n195              30.8             33.1   2.3  \n196              53.6             53.8   0.2  \n197              43.1             42.1  -1.0  \n\n[198 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>first_name</th>\n      <th>last_name</th>\n      <th>age</th>\n      <th>Happy_Sad_group</th>\n      <th>Dosage</th>\n      <th>Drug</th>\n      <th>Mem_Score_Before</th>\n      <th>Mem_Score_After</th>\n      <th>Diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Bastian</td>\n      <td>Carrasco</td>\n      <td>25</td>\n      <td>H</td>\n      <td>1</td>\n      <td>A</td>\n      <td>63.5</td>\n      <td>61.2</td>\n      <td>-2.3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Evan</td>\n      <td>Carrasco</td>\n      <td>52</td>\n      <td>S</td>\n      <td>1</td>\n      <td>A</td>\n      <td>41.6</td>\n      <td>40.7</td>\n      <td>-0.9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Florencia</td>\n      <td>Carrasco</td>\n      <td>29</td>\n      <td>H</td>\n      <td>1</td>\n      <td>A</td>\n      <td>59.7</td>\n      <td>55.1</td>\n      <td>-4.6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Holly</td>\n      <td>Carrasco</td>\n      <td>50</td>\n      <td>S</td>\n      <td>1</td>\n      <td>A</td>\n      <td>51.7</td>\n      <td>51.2</td>\n      <td>-0.5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Justin</td>\n      <td>Carrasco</td>\n      <td>52</td>\n      <td>H</td>\n      <td>1</td>\n      <td>A</td>\n      <td>47.0</td>\n      <td>47.1</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>193</th>\n      <td>Jacob</td>\n      <td>Novak</td>\n      <td>52</td>\n      <td>H</td>\n      <td>3</td>\n      <td>T</td>\n      <td>71.3</td>\n      <td>74.3</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>194</th>\n      <td>Teo</td>\n      <td>Steiner</td>\n      <td>41</td>\n      <td>S</td>\n      <td>3</td>\n      <td>T</td>\n      <td>72.5</td>\n      <td>70.4</td>\n      <td>-2.1</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>Alexander</td>\n      <td>Takahashi</td>\n      <td>54</td>\n      <td>S</td>\n      <td>3</td>\n      <td>T</td>\n      <td>30.8</td>\n      <td>33.1</td>\n      <td>2.3</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>Alexandere</td>\n      <td>Takahashi</td>\n      <td>40</td>\n      <td>H</td>\n      <td>3</td>\n      <td>T</td>\n      <td>53.6</td>\n      <td>53.8</td>\n      <td>0.2</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>Chloe</td>\n      <td>Takahashi</td>\n      <td>32</td>\n      <td>S</td>\n      <td>3</td>\n      <td>T</td>\n      <td>43.1</td>\n      <td>42.1</td>\n      <td>-1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>198 rows Ã— 9 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def onehot_encode(df, column):\n    df = df.copy()\n    \n    dummies = pd.get_dummies(df[column], prefix = column)\n    if len(df[column].unique()) == 2:\n        dummies = dummies.drop(dummies.columns[0], axis = 1)\n        \n    df = pd.concat([df, dummies], axis = 1)\n    df = df.drop(column, axis = 1)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:16:14.215331Z","iopub.execute_input":"2022-12-07T13:16:14.215730Z","iopub.status.idle":"2022-12-07T13:16:14.224075Z","shell.execute_reply.started":"2022-12-07T13:16:14.215697Z","shell.execute_reply":"2022-12-07T13:16:14.222564Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"def preprocess_inputs(df):\n    df = df.copy()\n    \n    ## One Hot encode categorical features:\n    for column in ['first_name', 'last_name', 'Happy_Sad_group']:\n        df = onehot_encode(df, column = column)\n    \n    ## Split the data \n    y = df['Drug']\n    X = df.drop('Drug', axis = 1)\n    \n    ## Train test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, shuffle = True, random_state = 1)\n    \n    ## Scale X\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    X_train = pd.DataFrame(scaler.transform(X_train), index = X_train.index, columns = X_train.columns)\n    X_test = pd.DataFrame(scaler.transform(X_test), index = X_test.index, columns = X_test.columns)\n    \n    return X_train, X_test, y_train, y_test","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:16:14.226557Z","iopub.execute_input":"2022-12-07T13:16:14.226968Z","iopub.status.idle":"2022-12-07T13:16:14.237495Z","shell.execute_reply.started":"2022-12-07T13:16:14.226933Z","shell.execute_reply":"2022-12-07T13:16:14.235928Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = preprocess_inputs(data)","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:16:14.240183Z","iopub.execute_input":"2022-12-07T13:16:14.241196Z","iopub.status.idle":"2022-12-07T13:16:14.274919Z","shell.execute_reply.started":"2022-12-07T13:16:14.241147Z","shell.execute_reply":"2022-12-07T13:16:14.273565Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:16:14.276255Z","iopub.execute_input":"2022-12-07T13:16:14.276635Z","iopub.status.idle":"2022-12-07T13:16:14.312863Z","shell.execute_reply.started":"2022-12-07T13:16:14.276602Z","shell.execute_reply":"2022-12-07T13:16:14.311691Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"          age    Dosage  Mem_Score_Before  Mem_Score_After      Diff  \\\n124 -0.302247  1.206716          0.249183        -0.151850 -0.594735   \n97   0.909251  0.025675          1.221038         1.038471 -0.092208   \n42   1.428464  0.025675          0.438505        -0.684661 -1.707473   \n17  -1.167603 -1.155366          0.413262         0.698379  0.518003   \n5   -0.215712 -1.155366          0.564720        -0.117841 -0.989577   \n..        ...       ...               ...              ...       ...   \n133 -0.215712 -1.155366         -0.665875        -0.752679 -0.244761   \n137  1.947677 -1.155366          2.432701         2.370498  0.293661   \n72   1.082322 -1.155366          0.552099         0.069210 -0.675498   \n140 -0.475319 -1.155366         -0.533349        -1.115444 -1.007525   \n37  -1.254139  0.025675         -0.470242        -0.610974 -0.298603   \n\n     first_name_Aaron  first_name_Adam  first_name_Ai  first_name_Akane  \\\n124               0.0        -0.121268            0.0         -0.121268   \n97                0.0        -0.121268            0.0         -0.121268   \n42                0.0        -0.121268            0.0         -0.121268   \n17                0.0        -0.121268            0.0         -0.121268   \n5                 0.0        -0.121268            0.0         -0.121268   \n..                ...              ...            ...               ...   \n133               0.0        -0.121268            0.0         -0.121268   \n137               0.0        -0.121268            0.0         -0.121268   \n72                0.0        -0.121268            0.0         -0.121268   \n140               0.0        -0.121268            0.0         -0.121268   \n37                0.0        -0.121268            0.0         -0.121268   \n\n     first_name_Akira  ...  last_name_Lopez  last_name_McCarthy  \\\n124               0.0  ...         2.761340            -0.23116   \n97                0.0  ...        -0.362143            -0.23116   \n42                0.0  ...        -0.362143            -0.23116   \n17                0.0  ...        -0.362143            -0.23116   \n5                 0.0  ...        -0.362143            -0.23116   \n..                ...  ...              ...                 ...   \n133               0.0  ...        -0.362143            -0.23116   \n137               0.0  ...        -0.362143            -0.23116   \n72                0.0  ...        -0.362143            -0.23116   \n140               0.0  ...        -0.362143            -0.23116   \n37                0.0  ...         2.761340            -0.23116   \n\n     last_name_Morin  last_name_Novak  last_name_Price  last_name_Rodriguez  \\\n124        -0.085436        -0.085436              0.0            -0.085436   \n97         -0.085436        -0.085436              0.0            -0.085436   \n42         -0.085436        -0.085436              0.0            -0.085436   \n17         11.704700        -0.085436              0.0            -0.085436   \n5          -0.085436        -0.085436              0.0            -0.085436   \n..               ...              ...              ...                  ...   \n133        -0.085436        -0.085436              0.0            -0.085436   \n137        -0.085436        -0.085436              0.0            -0.085436   \n72         -0.085436        -0.085436              0.0            -0.085436   \n140        -0.085436        -0.085436              0.0            -0.085436   \n37         -0.085436        -0.085436              0.0            -0.085436   \n\n     last_name_Steiner  last_name_Summers  last_name_Takahashi  \\\n124          -0.264135          -0.264135            -0.336011   \n97           -0.264135          -0.264135            -0.336011   \n42           -0.264135           3.785939            -0.336011   \n17           -0.264135          -0.264135            -0.336011   \n5            -0.264135          -0.264135            -0.336011   \n..                 ...                ...                  ...   \n133          -0.264135          -0.264135            -0.336011   \n137          -0.264135          -0.264135            -0.336011   \n72           -0.264135          -0.264135            -0.336011   \n140          -0.264135          -0.264135            -0.336011   \n37           -0.264135          -0.264135            -0.336011   \n\n     Happy_Sad_group_S  \n124                1.0  \n97                 1.0  \n42                 1.0  \n17                -1.0  \n5                  1.0  \n..                 ...  \n133                1.0  \n137               -1.0  \n72                 1.0  \n140                1.0  \n37                -1.0  \n\n[138 rows x 163 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>Dosage</th>\n      <th>Mem_Score_Before</th>\n      <th>Mem_Score_After</th>\n      <th>Diff</th>\n      <th>first_name_Aaron</th>\n      <th>first_name_Adam</th>\n      <th>first_name_Ai</th>\n      <th>first_name_Akane</th>\n      <th>first_name_Akira</th>\n      <th>...</th>\n      <th>last_name_Lopez</th>\n      <th>last_name_McCarthy</th>\n      <th>last_name_Morin</th>\n      <th>last_name_Novak</th>\n      <th>last_name_Price</th>\n      <th>last_name_Rodriguez</th>\n      <th>last_name_Steiner</th>\n      <th>last_name_Summers</th>\n      <th>last_name_Takahashi</th>\n      <th>Happy_Sad_group_S</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>124</th>\n      <td>-0.302247</td>\n      <td>1.206716</td>\n      <td>0.249183</td>\n      <td>-0.151850</td>\n      <td>-0.594735</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>2.761340</td>\n      <td>-0.23116</td>\n      <td>-0.085436</td>\n      <td>-0.085436</td>\n      <td>0.0</td>\n      <td>-0.085436</td>\n      <td>-0.264135</td>\n      <td>-0.264135</td>\n      <td>-0.336011</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>0.909251</td>\n      <td>0.025675</td>\n      <td>1.221038</td>\n      <td>1.038471</td>\n      <td>-0.092208</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.362143</td>\n      <td>-0.23116</td>\n      <td>-0.085436</td>\n      <td>-0.085436</td>\n      <td>0.0</td>\n      <td>-0.085436</td>\n      <td>-0.264135</td>\n      <td>-0.264135</td>\n      <td>-0.336011</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>1.428464</td>\n      <td>0.025675</td>\n      <td>0.438505</td>\n      <td>-0.684661</td>\n      <td>-1.707473</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.362143</td>\n      <td>-0.23116</td>\n      <td>-0.085436</td>\n      <td>-0.085436</td>\n      <td>0.0</td>\n      <td>-0.085436</td>\n      <td>-0.264135</td>\n      <td>3.785939</td>\n      <td>-0.336011</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-1.167603</td>\n      <td>-1.155366</td>\n      <td>0.413262</td>\n      <td>0.698379</td>\n      <td>0.518003</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.362143</td>\n      <td>-0.23116</td>\n      <td>11.704700</td>\n      <td>-0.085436</td>\n      <td>0.0</td>\n      <td>-0.085436</td>\n      <td>-0.264135</td>\n      <td>-0.264135</td>\n      <td>-0.336011</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-0.215712</td>\n      <td>-1.155366</td>\n      <td>0.564720</td>\n      <td>-0.117841</td>\n      <td>-0.989577</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.362143</td>\n      <td>-0.23116</td>\n      <td>-0.085436</td>\n      <td>-0.085436</td>\n      <td>0.0</td>\n      <td>-0.085436</td>\n      <td>-0.264135</td>\n      <td>-0.264135</td>\n      <td>-0.336011</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>133</th>\n      <td>-0.215712</td>\n      <td>-1.155366</td>\n      <td>-0.665875</td>\n      <td>-0.752679</td>\n      <td>-0.244761</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.362143</td>\n      <td>-0.23116</td>\n      <td>-0.085436</td>\n      <td>-0.085436</td>\n      <td>0.0</td>\n      <td>-0.085436</td>\n      <td>-0.264135</td>\n      <td>-0.264135</td>\n      <td>-0.336011</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>137</th>\n      <td>1.947677</td>\n      <td>-1.155366</td>\n      <td>2.432701</td>\n      <td>2.370498</td>\n      <td>0.293661</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.362143</td>\n      <td>-0.23116</td>\n      <td>-0.085436</td>\n      <td>-0.085436</td>\n      <td>0.0</td>\n      <td>-0.085436</td>\n      <td>-0.264135</td>\n      <td>-0.264135</td>\n      <td>-0.336011</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>1.082322</td>\n      <td>-1.155366</td>\n      <td>0.552099</td>\n      <td>0.069210</td>\n      <td>-0.675498</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.362143</td>\n      <td>-0.23116</td>\n      <td>-0.085436</td>\n      <td>-0.085436</td>\n      <td>0.0</td>\n      <td>-0.085436</td>\n      <td>-0.264135</td>\n      <td>-0.264135</td>\n      <td>-0.336011</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>140</th>\n      <td>-0.475319</td>\n      <td>-1.155366</td>\n      <td>-0.533349</td>\n      <td>-1.115444</td>\n      <td>-1.007525</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.362143</td>\n      <td>-0.23116</td>\n      <td>-0.085436</td>\n      <td>-0.085436</td>\n      <td>0.0</td>\n      <td>-0.085436</td>\n      <td>-0.264135</td>\n      <td>-0.264135</td>\n      <td>-0.336011</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>-1.254139</td>\n      <td>0.025675</td>\n      <td>-0.470242</td>\n      <td>-0.610974</td>\n      <td>-0.298603</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>-0.121268</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>2.761340</td>\n      <td>-0.23116</td>\n      <td>-0.085436</td>\n      <td>-0.085436</td>\n      <td>0.0</td>\n      <td>-0.085436</td>\n      <td>-0.264135</td>\n      <td>-0.264135</td>\n      <td>-0.336011</td>\n      <td>-1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>138 rows Ã— 163 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"models = {\n    \"                   Logistic Regression\": LogisticRegression(),\n    \"                   K-Nearest Neighbors\": KNeighborsClassifier(),\n    \"                         Decision Tree\": DecisionTreeClassifier(),\n    \"Support Vector Machine (Linear Kernel)\": LinearSVC(),\n    \"   Support Vector Machine (RBF Kernel)\": SVC(),\n    \"                        Neural Network\": MLPClassifier(),\n    \"                         Random Forest\": RandomForestClassifier(),\n    \"                     Gradient Boosting\": GradientBoostingClassifier(),\n#     \"                               XGBoost\": XGBClassifier(eval_metric='mlogloss'),\n    \"                              LightGBM\": LGBMClassifier(),\n    \"                              CatBoost\": CatBoostClassifier(verbose=0)\n}\n\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    print(name + \" trained.\")","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:16:33.053566Z","iopub.execute_input":"2022-12-07T13:16:33.053970Z","iopub.status.idle":"2022-12-07T13:16:36.232820Z","shell.execute_reply.started":"2022-12-07T13:16:33.053939Z","shell.execute_reply":"2022-12-07T13:16:36.231530Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"                   Logistic Regression trained.\n                   K-Nearest Neighbors trained.\n                         Decision Tree trained.\nSupport Vector Machine (Linear Kernel) trained.\n   Support Vector Machine (RBF Kernel) trained.\n                        Neural Network trained.\n                         Random Forest trained.\n                     Gradient Boosting trained.\n                              LightGBM trained.\n                              CatBoost trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"for name, model in models.items():\n    print(name + \": {:.2f}%\".format(model.score(X_test, y_test) * 100))","metadata":{"execution":{"iopub.status.busy":"2022-12-07T13:16:47.894150Z","iopub.execute_input":"2022-12-07T13:16:47.894596Z","iopub.status.idle":"2022-12-07T13:16:48.087672Z","shell.execute_reply.started":"2022-12-07T13:16:47.894559Z","shell.execute_reply":"2022-12-07T13:16:48.086241Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"                   Logistic Regression: 41.67%\n                   K-Nearest Neighbors: 36.67%\n                         Decision Tree: 45.00%\nSupport Vector Machine (Linear Kernel): 40.00%\n   Support Vector Machine (RBF Kernel): 43.33%\n                        Neural Network: 38.33%\n                         Random Forest: 43.33%\n                     Gradient Boosting: 50.00%\n                              LightGBM: 38.33%\n                              CatBoost: 48.33%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}